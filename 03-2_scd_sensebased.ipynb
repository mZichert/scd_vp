{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "710056ed-4ab3-4dac-9d5f-c8f3307c0806",
   "metadata": {},
   "source": [
    "## **SCD - Sense based**\n",
    "\n",
    "- AP and KM Clusters\n",
    "- Jensen Shannon Divergence\n",
    "- Shannon-Entropy\n",
    "- https://github.com/glnmario/cwr4lsc/blob/master/change_metrics.py\n",
    "- https://github.com/matejMartinc/scalable_semantic_shift/blob/master/measure_semantic_shift.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea4b0f0-bd9e-4ffa-94ee-5beeee53f526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain, groupby, combinations\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mtick\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "\n",
    "journals = [\"pr\", \"pra\", \"prb\", \"prc\", \"prd\", \"pre\", \"prl\", \"rmp\"]\n",
    "years = [year for year in range(1924, 2023)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3bdc07-20a2-4864-b5d6-2247d2c93448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cluster_df(target_word, journal, slice_width, emb_type, cluster_type):\n",
    "    if target_word == \"virtual\":\n",
    "        if cluster_type == \"k_means\" or cluster_type == \"k_means_filtered\":\n",
    "            cluster_type = \"km\"\n",
    "        if cluster_type == \"ap_cluster\" or cluster_type == \"ap_cluster_filtered\":\n",
    "            cluster_type = \"ap\"\n",
    "        return pd.read_pickle(f\"../../data/clustering/all_years/{emb_type}_{cluster_type}_clustering_{journal if journal else 'all'}.pkl\")\n",
    "\n",
    "def create_slice_df(data_df, year, slice_width, journal):\n",
    "    if journal:\n",
    "        return data_df.loc[(data_df.year >= year) & (data_df.year < year+slice_width) & (data_df.journal == journal)]\n",
    "    else:\n",
    "        return data_df.loc[(data_df.year >= year) & (data_df.year < year+slice_width)]\n",
    "    \n",
    "# probability distribution for time intervall\n",
    "def get_probability_distribution(slice_df, cluster_type, n_clusters):\n",
    "    pdist = slice_df[cluster_type].value_counts(normalize = True)\n",
    "    pdist = [pdist[cluster] if cluster in pdist else 0 for cluster in range(n_clusters)]\n",
    "    return pdist\n",
    "\n",
    "# compute jsd. P and Q are probability distribution over clusters\n",
    "# JSD is symetric so input order doesn't matter\n",
    "def compute_jsd(p, q):\n",
    "    p = np.asarray(p)\n",
    "    q = np.asarray(q)\n",
    "    m = (p + q) / 2\n",
    "    return (entropy(p, m) + entropy(q, m)) / 2\n",
    "\n",
    "# calculate entropy difference\n",
    "# P and Q are again probability distributions over clusters\n",
    "# Input order matters\n",
    "def compute_entropy_difference(p, q, n_clusters):\n",
    "    p = np.asarray(p)\n",
    "    q = np.asarray(q)\n",
    "    entropy_p = entropy(p)\n",
    "    entropy_q = entropy(q)\n",
    "    # normalize entropy on log(n_clusters)\n",
    "    entropy_p /= np.log(n_clusters)\n",
    "    entropy_q /= np.log(n_clusters)\n",
    "    entropy_diff = entropy_q - entropy_p\n",
    "    return entropy_diff\n",
    "\n",
    "def compute_normalized_entropy(p, n_clusters):\n",
    "    p = np.asarray(p)\n",
    "    entropy_p = entropy(p)\n",
    "    entropy_p /= np.log(n_clusters)\n",
    "    return entropy_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeac0500-d3b8-4ef3-be08-48ec8cb49162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "embedding_types = [\"token_emb\"] #[\"token_emb\", \"sentence_emb\"]\n",
    "cluster_types = [\"km\"] #[\"ap\", \"km\"]\n",
    "\n",
    "print(\"Clustering not done for:\\n\")\n",
    "\n",
    "for emb_type in embedding_types:\n",
    "    for cluster_type in cluster_types:\n",
    "        for journal in [\"all\"] + journals:\n",
    "            if not os.path.isfile(f\"../../data/clustering/all_years/{emb_type}_{cluster_type}_clustering_{journal}.pkl\"):\n",
    "                print(f\"{emb_type}_{cluster_type}_clustering_{journal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cb6a28-ace7-492c-9c85-df8c4f9b2937",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word = \"virtual\"\n",
    "\n",
    "cluster_types = [\"k_means\", \"k_means_filtered\"] #[\"k_means\", \"k_means_filtered\", \"ap_cluster\", \"ap_cluster_filtered\"]\n",
    "embedding_types = [\"token_emb\"] # [\"token_emb\", \"sentence_emb\"]\n",
    "time_slices = [1, 3, 5]\n",
    "\n",
    "for cluster_type in tqdm(cluster_types):\n",
    "    \n",
    "    for emb_type in tqdm(embedding_types): \n",
    "        \n",
    "        for slice_width in tqdm(time_slices):\n",
    "            \n",
    "            for journal in journals + [None]:\n",
    "                \n",
    "                #journal = None\n",
    "                \n",
    "                # Check if clustering is done\n",
    "                if cluster_type == \"k_means\" or cluster_type == \"k_means_filtered\":\n",
    "                    ct = \"km\"\n",
    "                if cluster_type == \"ap_cluster\" or cluster_type == \"ap_cluster_filtered\":\n",
    "                    ct = \"ap\"\n",
    "                if not os.path.isfile(f\"../../data/clustering/all_years/{emb_type}_{ct}_clustering_{journal if journal else 'all'}.pkl\"):\n",
    "                    print(f\"{emb_type}_{ct}_clustering_{journal if journal else 'all'}\")\n",
    "                    continue\n",
    "\n",
    "                cluster_df = load_cluster_df(target_word, journal, slice_width, emb_type, cluster_type)\n",
    "                n_clusters=cluster_df[cluster_type].unique().size\n",
    "                if journal:\n",
    "                    year_start = cluster_df.loc[cluster_df.journal == journal].year.min()\n",
    "                    year_end = cluster_df.loc[cluster_df.journal == journal].year.max()\n",
    "                else:\n",
    "                    year_start = cluster_df.year.min()\n",
    "                    year_end = cluster_df.year.max()\n",
    "\n",
    "                jsd_dict = {}\n",
    "                ed_dict = {}\n",
    "                entropy_dict = {}\n",
    "\n",
    "                # Initialize first time step\n",
    "                slice_df = create_slice_df(cluster_df, year_start, slice_width, journal) \n",
    "                p = get_probability_distribution(slice_df, cluster_type, n_clusters)\n",
    "\n",
    "                for year in range(year_start+slice_width, year_end+1, slice_width):\n",
    "\n",
    "                    slice_df = create_slice_df(cluster_df, year, slice_width, journal)\n",
    "                    q = get_probability_distribution(slice_df, cluster_type, n_clusters)\n",
    "\n",
    "                    jsd = compute_jsd(p, q)\n",
    "                    #ed = compute_entropy_difference(p, q, n_clusters)\n",
    "                    entropy_p = compute_normalized_entropy(p, n_clusters)\n",
    "\n",
    "                    jsd_dict[f\"{year}_{year+slice_width-1}\"] = jsd\n",
    "                    #ed_dict[f\"{year}_{year+slice_width-1}\"] = ed\n",
    "                    entropy_dict[f\"{year}_{year+slice_width-1}\"] = entropy_p\n",
    "\n",
    "                    p = q\n",
    "                    \n",
    "                jsd_df = pd.DataFrame.from_dict(jsd_dict, orient=\"index\", columns = [f\"{cluster_type}_{emb_type}_{journal if journal else 'all'}_{slice_width}_jsd\"])\n",
    "                jsd_df.to_pickle(f\"../../data/scd/sb/jsd/{target_word}_jsd_sb_{cluster_type}_{emb_type}_{journal if journal else 'all'}_{slice_width}.pkl\")\n",
    "                \n",
    "                #ed_df = pd.DataFrame.from_dict(ed_dict, orient=\"index\", columns = [f\"{cluster_type}_{emb_type}_{journal if journal else 'all'}_{slice_width}_ed\"])\n",
    "                #ed_df.to_pickle(f\"../../data/scd/sb/ed/{target_word}_ed_sb_{cluster_type}_{emb_type}_{journal if journal else 'all'}_{slice_width}.pkl\")\n",
    "                \n",
    "                entropy_df = pd.DataFrame.from_dict(entropy_dict, orient=\"index\", columns = [f\"{cluster_type}_{emb_type}_{journal if journal else 'all'}_{slice_width}_entropy\"])\n",
    "                entropy_df.to_pickle(f\"../../data/scd/sb/entropy/{target_word}_entropy_sb_{cluster_type}_{emb_type}_{journal if journal else 'all'}_{slice_width}.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scdvpVenv",
   "language": "python",
   "name": "scdvpvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
